import collections
import re

Token = collections.namedtuple('Token', ['typ', 'value', 'line', 'column','depth'])

def tokenize(s):
    keywords      = {'if', 'function', 'private', 'for', 'var', 'return'}
    keywordsTypes = {'integer', 'charstring'}

    token_specification = [
        ('NUMBER',  r'\d+(\.\d*)?'), # Integer or decimal number
        ('ASSIGN',  r':='),          # Assignment operator
        ('END',     r';'),           # Statement terminator
        ('BLOCKSTART', r'{'),           # start new block of code
        ('BLOCKEND',  r'}'),            # end block of code
        ('STRING',  r'"([^"\\]|\\.)*"'),# string (can contain quotes)
        ('ID',      r'[A-Za-z_]+'),  # Identifiers
        ('COMMENT1',r'\/\/[^\n]*'),  # single line comment
        ('COMMENT2',r'/\*.+?\*/'),   # multiline comment
        ('OP',      r'[+*\/\-]'),    # Arithmetic operators
        ('NEWLINE', r'\n'),          # Line endings
        ('SKIP',    r'[ \t]')        # Skip over spaces and tabs
    ]
    tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)

    line = 1
    pos = line_start = 0
    depth = 0
    get_token = re.compile(tok_regex,re.DOTALL).match
    mo = get_token(s)
    while mo is not None:
        typ = mo.lastgroup

        if typ == 'BLOCKSTART':
           depth=depth+1

        if typ == 'NEWLINE':
            line_start = pos
            line += 1

        elif typ != 'SKIP':
            val = mo.group(typ)
            if typ == 'ID' and val in keywords:
                typ = 'KEYWORD'
            if type == 'ID' and val in keywordsTypes:
                typ = 'TYPE'
            yield Token(typ, val, line, mo.start()-line_start,depth)

        if typ == 'BLOCKEND':
           depth=depth-1

        pos = mo.end()
        mo = get_token(s, pos)
    if pos != len(s):
        raise RuntimeError('Unexpected character %r on line %d' %(s[pos], line))



statements = '''
function WORD1
{
WORD //commented word
/*aa
 */
 "this is a \\"string\\""
 {
        a := b + c * d;
        e := f * 0.05;
 }
}
'''

print "input:",statements,"\n\n"

for token in tokenize(statements):
    for i in range(0,token.depth):
      print " ",
    print(token)
